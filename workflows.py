# workflows.py

from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains.summarize import load_summarize_chain
from langchain_core.documents import Document
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from typing import List, Dict, Any
import asyncio

from config import AppSettings, LlmProviderSettings

def get_llm(llm_settings: LlmProviderSettings) -> ChatOpenAI:
    """æ ¹æ®é…ç½®åˆå§‹åŒ–å¹¶è¿”å›ä¸€ä¸ªChatOpenAIå®ä¾‹"""
    return ChatOpenAI(
        model=llm_settings.model_name,
        temperature=llm_settings.temperature,
        api_key=llm_settings.api_key,
        base_url=llm_settings.base_url
    )

def load_and_split_documents(settings: AppSettings) -> List[Document]:
    """åŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£"""
    # åˆ›å»ºä¸¤ä¸ªåŠ è½½å™¨ï¼šä¸€ä¸ªç”¨äºtxtæ–‡ä»¶ï¼Œä¸€ä¸ªç”¨äºmdæ–‡ä»¶
    txt_loader = DirectoryLoader(
        settings.paths.input_dir,
        glob="*.txt",  # è¯»å–txtæ–‡ä»¶
        show_progress=True,
        use_multithreading=True
    )
    
    md_loader = DirectoryLoader(
        settings.paths.input_dir,
        glob="*.md",  # è¯»å–mdæ–‡ä»¶
        show_progress=True,
        use_multithreading=True
    )
    
    # åŠ è½½æ‰€æœ‰æ–‡æ¡£
    txt_documents = txt_loader.load()
    md_documents = md_loader.load()
    documents = txt_documents + md_documents
    
    # éªŒè¯åŠ è½½çš„æ–‡æ¡£å†…å®¹
    for doc in documents:
        print(f"ğŸ“„ åŠ è½½æ–‡æ¡£: {doc.metadata.get('source', 'unknown')}")
        print(f"   å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
        print(f"   å†…å®¹é¢„è§ˆ: {doc.page_content[:200]}...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.processing.chunk_size,
        chunk_overlap=settings.processing.chunk_overlap
    )
    return text_splitter.split_documents(documents)


def run_single_round_analysis(docs: List[Document], prompt_template: ChatPromptTemplate, llm: ChatOpenAI) -> List[Dict[str, str]]:
    """
    è¿è¡Œå•è½®åˆ†æå·¥ä½œæµã€‚
    å¯¹æ¯ä¸ªæ–‡æ¡£åˆ†åˆ«è¿›è¡Œåˆ†æï¼Œè¿”å›æ¯ä¸ªæ–‡ä»¶çš„å•ç‹¬ç»“æœã€‚
    """
    file_results = []
    
    # æŒ‰æ–‡æ¡£æºæ–‡ä»¶åˆ†ç»„
    docs_by_source = {}
    for doc in docs:
        source = doc.metadata.get('source', 'unknown')
        if source not in docs_by_source:
            docs_by_source[source] = []
        docs_by_source[source].append(doc)
    
    print(f"ğŸ“ å‘ç° {len(docs_by_source)} ä¸ªæºæ–‡ä»¶")
    
    # å¯¹æ¯ä¸ªæºæ–‡ä»¶åˆ†åˆ«å¤„ç†
    for source, source_docs in docs_by_source.items():
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {source}")
        
        # åˆå¹¶åŒä¸€æ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£å—
        combined_text = "\n\n".join([doc.page_content for doc in source_docs])
        print(f"   åˆå¹¶åæ–‡æœ¬é•¿åº¦: {len(combined_text)} å­—ç¬¦")
        print(f"   æ–‡æœ¬é¢„è§ˆ: {combined_text[:300]}...")
        
        # åˆ›å»ºåŒ…å«åˆå¹¶æ–‡æœ¬çš„å®Œæ•´prompt
        full_prompt = prompt_template.format(text=combined_text)
        print(f"   å®Œæ•´prompté•¿åº¦: {len(str(full_prompt))} å­—ç¬¦")
        
        # ç›´æ¥è°ƒç”¨LLM
        print(f"   æ­£åœ¨è°ƒç”¨LLM...")
        response = llm.invoke(full_prompt)
        file_result = response.content
        print(f"   LLMè¿”å›ç»“æœé•¿åº¦: {len(file_result)} å­—ç¬¦")
        print(f"   ç»“æœé¢„è§ˆ: {file_result[:300]}...")
        
        # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
        file_results.append({
            "source": source,
            "result": file_result
        })
    
    return file_results


def run_multi_round_analysis(
    docs: List[Document],
    round_prompts: List[ChatPromptTemplate],
    llm: ChatOpenAI
) -> List[Dict[str, Any]]:
    """
    è¿è¡Œå¤šè½®ã€é€’è¿›å¼åˆ†æå·¥ä½œæµã€‚
    æ¯ä¸€è½®éƒ½ä»¥ä¸Šä¸€è½®çš„ç»“æœä¸ºåŸºç¡€ï¼Œä¿æŒå®Œæ•´çš„å¯¹è¯è®°å¿†ã€‚
    """
    analysis_history = []
    
    # åˆ›å»ºå¯¹è¯è®°å¿†
    memory = ConversationBufferMemory()
    conversation = ConversationChain(
        llm=llm,
        memory=memory,
        verbose=False
    )
    
    for i, prompt_template in enumerate(round_prompts):
        print(f"\n===== å¼€å§‹ç¬¬ {i+1} è½®åˆ†æ... =====")
        
        if i == 0:
            # ç¬¬ä¸€è½®ï¼šä½¿ç”¨æ ‡å‡†çš„map_reduceé“¾å¤„ç†æ–‡æ¡£
            round_results = run_single_round_analysis(docs, prompt_template, llm)
            # å°†å¤šä¸ªæ–‡ä»¶çš„ç»“æœåˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
            round_result_text = "\n\n".join([result["result"] for result in round_results])
            
            # è·å–promptæ¨¡æ¿çš„åŸå§‹æ–‡æœ¬å†…å®¹
            prompt_text = ""
            for message in prompt_template.messages:
                if hasattr(message, 'prompt'):
                    prompt_text += message.prompt.template
                elif hasattr(message, 'template'):
                    prompt_text += message.template
            
            # ä½¿ç”¨ç”¨æˆ·æä¾›çš„promptæ¨¡æ¿ï¼Œå°†æ–‡æ¡£å†…å®¹æ›¿æ¢{text}å˜é‡
            current_prompt = prompt_text.replace('{text}', round_result_text).replace('{previous_results}', '')
            print(f"   ç¬¬ä¸€è½®Prompté¢„è§ˆ: {current_prompt[:300]}...")
            response = conversation.predict(input=current_prompt)
            round_result_text = response
            
        else:
            # ç¬¬äºŒè½®å’Œç¬¬ä¸‰è½®ï¼šä½¿ç”¨å¯¹è¯é“¾ï¼Œä¿æŒå¯¹è¯è®°å¿†
            # è·å–promptæ¨¡æ¿çš„åŸå§‹æ–‡æœ¬å†…å®¹
            prompt_text = ""
            for message in prompt_template.messages:
                if hasattr(message, 'prompt'):
                    prompt_text += message.prompt.template
                elif hasattr(message, 'template'):
                    prompt_text += message.template
            
            # åˆ›å»ºå½“å‰è½®æ¬¡çš„æç¤º
            current_prompt = prompt_text.replace('{previous_results}', '').replace('{text}', '')
            
            # ä½¿ç”¨å¯¹è¯é“¾ï¼ŒLLMä¼šè‡ªåŠ¨è®°ä½ä¹‹å‰çš„å¯¹è¯å†å²
            response = conversation.predict(input=current_prompt)
            round_result_text = response
        
        # è®°å½•æœ¬è½®ç»“æœ
        round_info = {
            "round": i + 1,
            "prompt_template": str(prompt_template.messages),
            "result": round_result_text,
            "conversation_history": memory.chat_memory.messages  # ä¿å­˜å¯¹è¯å†å²
        }
        analysis_history.append(round_info)
        
        print(f"   å¯¹è¯å†å²é•¿åº¦: {len(memory.chat_memory.messages)} æ¡æ¶ˆæ¯")
        print(f"   å½“å‰è½®æ¬¡ç»“æœé•¿åº¦: {len(round_result_text)} å­—ç¬¦")
        print(f"   ç»“æœé¢„è§ˆ: {round_result_text[:300]}...")
    
    return analysis_history


def run_multi_round_analysis_without_memory(
    docs: List[Document],
    round_prompts: List[ChatPromptTemplate],
    llm: ChatOpenAI,
    max_concurrent: int = 5
) -> List[Dict[str, Any]]:
    """
    è¿è¡Œå¤šè½®ã€é€’è¿›å¼åˆ†æå·¥ä½œæµï¼Œä¸ä½¿ç”¨å¯¹è¯è®°å¿†ã€‚
    é€šè¿‡æ˜¾å¼ä¼ é€’ä¹‹å‰è½®æ¬¡çš„ç»“æœæ¥å®ç°å¤šè½®åˆ†æã€‚
    å¯¹æ¯ä¸ªæ–‡ä»¶åˆ†åˆ«è¿›è¡Œå¤„ç†ï¼Œæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„ç»“æœã€‚
    
    Args:
        docs: æ–‡æ¡£åˆ—è¡¨
        round_prompts: æ¯è½®çš„æç¤ºæ¨¡æ¿
        llm: LLMå®ä¾‹
    """
    # 1) æŒ‰æ–‡æ¡£æºæ–‡ä»¶åˆ†ç»„ï¼Œå¹¶å‡†å¤‡æ¯ä¸ªæ–‡ä»¶çš„åˆå¹¶æ–‡æœ¬ä¸çŠ¶æ€
    docs_by_source: Dict[str, List[Document]] = {}
    for doc in docs:
        source = doc.metadata.get('source', 'unknown')
        if source not in docs_by_source:
            docs_by_source[source] = []
        docs_by_source[source].append(doc)

    print(f"ğŸ“ å‘ç° {len(docs_by_source)} ä¸ªæºæ–‡ä»¶ï¼Œå°†æ¯è½®å¹¶è¡Œå¤„ç†å„æ–‡ä»¶ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_concurrent}ï¼‰")

    file_states: Dict[str, Dict[str, Any]] = {}
    for source, source_docs in docs_by_source.items():
        combined_text = "\n\n".join([doc.page_content for doc in source_docs])
        print(f"\nğŸ“„ å‡†å¤‡æ–‡ä»¶: {source}")
        print(f"   åˆå¹¶åæ–‡æœ¬é•¿åº¦: {len(combined_text)} å­—ç¬¦")
        file_states[source] = {
            "combined_text": combined_text,
            "previous_results": [],
            "analysis_history": []
        }



    # 4) æŒ‰æ‰¹æ¬¡å¤„ç†ï¼šæ¯ä¸ªæ‰¹æ¬¡å¤„ç†å®Œæ‰€æœ‰è½®æ¬¡åå†å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
    sources = list(file_states.keys())
    num_rounds = len(round_prompts)
    
    # åˆ†æ‰¹å¤„ç†ï¼Œæ§åˆ¶å¹¶å‘æ•°é‡
    for batch_start in range(0, len(sources), max_concurrent):
        batch_sources = sources[batch_start:batch_start + max_concurrent]
        batch_num = batch_start // max_concurrent + 1
        print(f"\n===== å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch_sources)} ä¸ªæ–‡ä»¶ =====")
        
        # ä¸ºå½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶å¤„ç†æ‰€æœ‰è½®æ¬¡
        async def process_batch_all_rounds() -> Dict[str, List[Dict[str, Any]]]:
            batch_results = {}
            
            # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºä»»åŠ¡ï¼šå¤„ç†æ‰€æœ‰è½®æ¬¡
            async def process_file_all_rounds(source: str) -> List[Dict[str, Any]]:
                file_analysis_history = []
                file_previous_results = []
                
                for round_index in range(num_rounds):
                    prompt_template = round_prompts[round_index]
                    
                    # æ„å»ºå½“å‰è½®æ¬¡çš„ prompt
                    prompt_text = ""
                    for message in prompt_template.messages:
                        if hasattr(message, 'prompt'):
                            prompt_text += message.prompt.template
                        elif hasattr(message, 'template'):
                            prompt_text += message.template
                    
                    if round_index == 0:
                        current_prompt = prompt_text.replace('{text}', file_states[source]["combined_text"]).replace('{previous_results}', '')
                        print(f"   ğŸ“„ {source} ç¬¬{round_index+1}è½®Prompté¢„è§ˆ: {current_prompt[:100]}...")
                    else:
                        current_prompt = prompt_text.replace('{text}', file_states[source]["combined_text"])
                        
                        # æ›¿æ¢ç‰¹å®šè½®æ¬¡å˜é‡
                        for j, prev_result in enumerate(file_previous_results):
                            round_var = f"{{round{j+1}_result}}"
                            if round_var in current_prompt:
                                current_prompt = current_prompt.replace(round_var, prev_result)
                        
                        # æ›¿æ¢é€šç”¨å˜é‡
                        if "{previous_results}" in current_prompt:
                            previous_results_text = ""
                            for j, prev_result in enumerate(file_previous_results):
                                round_types = ["åŸºç¡€åˆ†æ", "æ·±åº¦åˆ†æ", "ç»¼åˆæ€»ç»“", "æœ€ç»ˆå»ºè®®", "è¡¥å……åˆ†æ"]
                                round_type = round_types[j] if j < len(round_types) else f"ç¬¬{j+1}è½®"
                                previous_results_text += f"\n{'='*60}\n"
                                previous_results_text += f"ğŸ“‹ ç¬¬ {j+1} è½®åˆ†æç»“æœ ({round_type})\n"
                                previous_results_text += f"{'='*60}\n"
                                previous_results_text += f"{prev_result}\n"
                                previous_results_text += f"\n{'='*60}\n"
                            current_prompt = current_prompt.replace('{previous_results}', previous_results_text)
                        
                        print(f"   ğŸ“„ {source} ç¬¬{round_index+1}è½®Prompté¢„è§ˆ: {current_prompt[:100]}...")
                    
                    # è°ƒç”¨LLMï¼ˆå«é‡è¯•ï¼‰
                    max_retries = 10
                    retry_count = 0
                    round_result_text = ""
                    
                    while retry_count < max_retries:
                        try:
                            response = await llm.ainvoke(current_prompt)
                            round_result_text = response.content
                        except Exception as e:
                            retry_count += 1
                            if retry_count >= max_retries:
                                print(f"   âŒ {source} ç¬¬{round_index+1}è½®é‡è¯•ä¸Šé™ï¼Œå¼‚å¸¸: {e}")
                                round_result_text = f"[å‘ç”Ÿå¼‚å¸¸ï¼š{e}]"
                                break
                            print(f"   âš ï¸ {source} ç¬¬{round_index+1}è½®ç¬¬{retry_count}æ¬¡é‡è¯•ï¼šè¯·æ±‚å¼‚å¸¸ï¼Œé‡è¯•ä¸­...")
                            continue
                        
                        if len(round_result_text.strip()) == 0:
                            retry_count += 1
                            print(f"   âš ï¸ {source} ç¬¬{round_index+1}è½®ç¬¬{retry_count}æ¬¡é‡è¯•ï¼šç»“æœä¸ºç©ºï¼Œé‡è¯•ä¸­...")
                            if retry_count >= max_retries:
                                print(f"   âŒ {source} ç¬¬{round_index+1}è½®ç»{max_retries}æ¬¡é‡è¯•ä»ä¸ºç©º")
                                round_result_text = f"[ç»“æœä¸ºç©ºï¼šç»è¿‡{max_retries}æ¬¡é‡è¯•ä»æ— ç»“æœ]"
                                break
                            continue
                        
                        break
                    
                    # è®°å½•æœ¬è½®ç»“æœ
                    round_types = ["åŸºç¡€åˆ†æ", "æ·±åº¦åˆ†æ", "ç»¼åˆæ€»ç»“", "æœ€ç»ˆå»ºè®®", "è¡¥å……åˆ†æ"]
                    round_type = round_types[round_index] if round_index < len(round_types) else f"ç¬¬{round_index+1}è½®"
                    round_info = {
                        "round": round_index + 1,
                        "round_type": round_type,
                        "result": round_result_text
                    }
                    
                    file_analysis_history.append(round_info)
                    file_previous_results.append(round_result_text)
                    
                    print(f"   âœ… {source} ç¬¬{round_index+1}è½®å®Œæˆï¼Œç»“æœé•¿åº¦: {len(round_result_text)}")
                
                return file_analysis_history
            
            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶
            batch_tasks = [process_file_all_rounds(source) for source in batch_sources]
            batch_results_list = await asyncio.gather(*batch_tasks)
            
            # æ”¶é›†ç»“æœ
            for source, analysis_history in zip(batch_sources, batch_results_list):
                batch_results[source] = analysis_history
            
            return batch_results
        
        # å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰è½®æ¬¡
        batch_results: Dict[str, List[Dict[str, Any]]] = asyncio.run(process_batch_all_rounds())
        
        # æ›´æ–°æ–‡ä»¶çŠ¶æ€å¹¶ç«‹å³ä¿å­˜å½“å‰æ‰¹æ¬¡çš„ç»“æœ
        for source, analysis_history in batch_results.items():
            file_states[source]["analysis_history"] = analysis_history
            file_states[source]["previous_results"] = [round_info["result"] for round_info in analysis_history]
            print(f"âœ… æ‰¹æ¬¡ {batch_num} æ–‡ä»¶ {source} æ‰€æœ‰è½®æ¬¡å¤„ç†å®Œæˆ")
        
        # ç«‹å³ä¿å­˜å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶ç»“æœ
        try:
            from main import save_file_results
            from config import settings
            output_path = settings.paths.output_dir / "multi_round_no_memory"
            
            for source in batch_sources:
                file_result = {
                    "source_file": source,
                    "analysis_history": file_states[source]["analysis_history"]
                }
                save_file_results(output_path, file_result)
                print(f"ğŸ’¾ æ‰¹æ¬¡ {batch_num} æ–‡ä»¶ {source} ç»“æœå·²ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ‰¹æ¬¡ {batch_num} ç»“æœæ—¶å‡ºé”™: {e}")
        
        print(f"ğŸ‰ æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œå…±å¤„ç† {len(batch_sources)} ä¸ªæ–‡ä»¶çš„æ‰€æœ‰ {num_rounds} è½®åˆ†æï¼Œç»“æœå·²ä¿å­˜")

    # 5) ç»„ç»‡è¾“å‡ºï¼ˆç»“æœå·²åœ¨æ¯ä¸ªæ‰¹æ¬¡å¤„ç†å®Œåä¿å­˜ï¼‰
    all_file_results: List[Dict[str, Any]] = []
    for source, state in file_states.items():
        file_result = {
            "source_file": source,
            "analysis_history": state["analysis_history"]
        }
        all_file_results.append(file_result)

    print("âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼ˆæ— è®°å¿†æ¨¡å¼ï¼Œå¹¶è¡ŒæŒ‰æ–‡ä»¶ï¼Œæ¯æ‰¹æ¬¡å·²ä¿å­˜ï¼‰")
    return all_file_results


def run_multi_round_analysis_with_memory(
    docs: List[Document],
    round_prompts: List[ChatPromptTemplate],
    llm: ChatOpenAI,
    memory_type: str = "buffer",
    max_token_limit: int = 4000
) -> List[Dict[str, Any]]:
    """
    è¿è¡Œå¤šè½®ã€é€’è¿›å¼åˆ†æå·¥ä½œæµï¼Œæ”¯æŒä¸åŒç±»å‹çš„å¯¹è¯è®°å¿†ã€‚
    å¯¹æ¯ä¸ªæ–‡ä»¶åˆ†åˆ«è¿›è¡Œå¤„ç†ï¼Œæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆç‹¬ç«‹çš„ç»“æœã€‚
    
    Args:
        docs: æ–‡æ¡£åˆ—è¡¨
        round_prompts: æ¯è½®çš„æç¤ºæ¨¡æ¿
        llm: LLMå®ä¾‹
        memory_type: è®°å¿†ç±»å‹ ("buffer", "window", "summary")
        max_token_limit: æœ€å¤§tokené™åˆ¶
    """
    # æŒ‰æ–‡æ¡£æºæ–‡ä»¶åˆ†ç»„
    docs_by_source = {}
    for doc in docs:
        source = doc.metadata.get('source', 'unknown')
        if source not in docs_by_source:
            docs_by_source[source] = []
        docs_by_source[source].append(doc)
    
    print(f"ğŸ“ å‘ç° {len(docs_by_source)} ä¸ªæºæ–‡ä»¶ï¼Œå°†åˆ†åˆ«å¤„ç†æ¯ä¸ªæ–‡ä»¶")
    
    all_file_results = []
    
    # å¯¹æ¯ä¸ªæºæ–‡ä»¶åˆ†åˆ«å¤„ç†
    for source, source_docs in docs_by_source.items():
        print(f"\nğŸ“„ æ­£åœ¨å¤„ç†æ–‡ä»¶: {source}")
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„å¯¹è¯è®°å¿†
        if memory_type == "buffer":
            from langchain.memory import ConversationBufferMemory
            memory = ConversationBufferMemory()
        elif memory_type == "window":
            from langchain.memory import ConversationBufferWindowMemory
            memory = ConversationBufferWindowMemory(k=5)
        elif memory_type == "summary":
            from langchain.memory import ConversationSummaryMemory
            memory = ConversationSummaryMemory(llm=llm)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è®°å¿†ç±»å‹: {memory_type}")
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºç‹¬ç«‹çš„å¯¹è¯é“¾
        conversation = ConversationChain(
            llm=llm,
            memory=memory,
            verbose=False
        )
        
        file_analysis_history = []
        
        # åˆå¹¶åŒä¸€æ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£å—
        combined_text = "\n\n".join([doc.page_content for doc in source_docs])
        print(f"   åˆå¹¶åæ–‡æœ¬é•¿åº¦: {len(combined_text)} å­—ç¬¦")
        
        for i, prompt_template in enumerate(round_prompts):
            print(f"   ===== å¼€å§‹ç¬¬ {i+1} è½®åˆ†æ... =====")
            
            # è·å–promptæ¨¡æ¿çš„åŸå§‹æ–‡æœ¬å†…å®¹
            prompt_text = ""
            for message in prompt_template.messages:
                if hasattr(message, 'prompt'):
                    prompt_text += message.prompt.template
                elif hasattr(message, 'template'):
                    prompt_text += message.template
            
            if i == 0:
                # ç¬¬ä¸€è½®ï¼šä½¿ç”¨ç”¨æˆ·æä¾›çš„promptæ¨¡æ¿ï¼Œå°†æ–‡æ¡£å†…å®¹æ›¿æ¢{text}å˜é‡
                current_prompt = prompt_text.replace('{text}', combined_text).replace('{previous_results}', '')
                print(f"   ç¬¬ä¸€è½®Prompté¢„è§ˆ: {current_prompt[:30]}...")
                response = conversation.predict(input=current_prompt)
                round_result_text = response
                
            else:
                # åç»­è½®æ¬¡ï¼šä½¿ç”¨å¯¹è¯è®°å¿†ï¼Œç§»é™¤{text}å’Œ{previous_results}å˜é‡
                current_prompt = prompt_text.replace('{previous_results}', '').replace('{text}', '')
                print(f"   ç¬¬{i+1}è½®Prompté¢„è§ˆ: {current_prompt[:30]}...")
                response = conversation.predict(input=current_prompt)
                round_result_text = response
            
            # æ£€æŸ¥ç»“æœé•¿åº¦ï¼Œå¦‚æœä¸ºç©ºåˆ™é‡è¯•
            max_retries = 3
            retry_count = 0
            original_result = round_result_text
            
            while retry_count < max_retries and len(round_result_text.strip()) == 0:
                retry_count += 1
                print(f"   âš ï¸ ç¬¬{retry_count}æ¬¡é‡è¯•ï¼šç¬¬{i+1}è½®ç»“æœä¸ºç©ºï¼Œé‡æ–°è¿è¡Œ...")
                response = conversation.predict(input=current_prompt)
                round_result_text = response.content
                
                if retry_count >= max_retries and len(round_result_text.strip()) == 0:
                    print(f"   âŒ ç¬¬{i+1}è½®åˆ†æå¤±è´¥ï¼šç»è¿‡{max_retries}æ¬¡é‡è¯•ä»æ— ç»“æœ")
                    round_result_text = f"[ç¬¬{i+1}è½®åˆ†æå¤±è´¥ï¼šLLMè¿”å›ç©ºç»“æœ]"
            
            # è®°å½•æœ¬è½®ç»“æœï¼ˆé¿å…JSONåºåˆ—åŒ–é—®é¢˜ï¼‰
            round_info = {
                "round": i + 1,
                "result": round_result_text,
                "memory_type": memory_type
            }
            
            # å®‰å…¨åœ°å¤„ç†å¯¹è¯å†å²ï¼Œé¿å…JSONåºåˆ—åŒ–é—®é¢˜
            if hasattr(memory, 'chat_memory') and memory.chat_memory.messages:
                # å°†Messageå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
                conversation_history = []
                for msg in memory.chat_memory.messages:
                    conversation_history.append({
                        "type": msg.__class__.__name__,
                        "content": msg.content
                    })
                round_info["conversation_history"] = conversation_history
                print(f"   å¯¹è¯å†å²é•¿åº¦: {len(conversation_history)} æ¡æ¶ˆæ¯")
            else:
                round_info["conversation_history"] = "æ‘˜è¦è®°å¿†" if memory_type == "summary" else []
                print(f"   ä½¿ç”¨{memory_type}è®°å¿†")
            
            file_analysis_history.append(round_info)
            print(f"   å½“å‰è½®æ¬¡ç»“æœé•¿åº¦: {len(round_result_text)} å­—ç¬¦")
            print(f"   ç»“æœé¢„è§ˆ: {round_result_text[:200]}...")
        
        # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
        file_result = {
            "source_file": source,
            "memory_type": memory_type,
            "analysis_history": file_analysis_history
        }
        all_file_results.append(file_result)
        
        # ç«‹å³ä¿å­˜å½“å‰æ–‡ä»¶çš„ç»“æœ
        try:
            from main import save_file_results
            from pathlib import Path
            from config import settings
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = settings.paths.output_dir / f"multi_round_{memory_type}"
            save_file_results(output_path, file_result)
            print(f"ğŸ’¾ æ–‡ä»¶ {source} ç»“æœå·²ç«‹å³ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡ä»¶ {source} æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… æ–‡ä»¶ {source} å¤„ç†å®Œæˆ")
    
    return all_file_results


def example_usage_with_memory():
    """
    ä½¿ç”¨ç¤ºä¾‹ï¼šå±•ç¤ºå¦‚ä½•ä½¿ç”¨ä¸åŒçš„å¯¹è¯è®°å¿†ç±»å‹
    """
    from config import AppSettings, LlmProviderSettings
    
    # é…ç½®è®¾ç½®
    settings = AppSettings()
    llm_settings = LlmProviderSettings()
    llm = get_llm(llm_settings)
    
    # åŠ è½½æ–‡æ¡£
    docs = load_and_split_documents(settings)
    
    # å®šä¹‰å¤šè½®æç¤ºæ¨¡æ¿
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.messages import HumanMessage, SystemMessage
    
    # ç¬¬ä¸€è½®ï¼šåŸºç¡€åˆ†æ
    round1_prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆã€‚è¯·ä»”ç»†åˆ†ææä¾›çš„æ–‡æ¡£å†…å®¹ã€‚"),
        HumanMessage(content="è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼š\n\n{text}")
    ])
    
    # ç¬¬äºŒè½®ï¼šæ·±å…¥åˆ†æ
    round2_prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="åŸºäºä¹‹å‰çš„åˆ†æï¼Œè¯·è¿›è¡Œæ›´æ·±å…¥çš„åˆ†æã€‚"),
        HumanMessage(content="è¯·åŸºäºä¹‹å‰çš„åˆ†æç»“æœï¼Œè¿›ä¸€æ­¥æ·±å…¥åˆ†ææ–‡æ¡£ä¸­çš„å…³é”®é—®é¢˜å’Œè¶‹åŠ¿ã€‚")
    ])
    
    # ç¬¬ä¸‰è½®ï¼šæ€»ç»“å’Œå»ºè®®
    round3_prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="åŸºäºæ‰€æœ‰ä¹‹å‰çš„åˆ†æï¼Œè¯·æä¾›æ€»ç»“å’Œå»ºè®®ã€‚"),
        HumanMessage(content="è¯·æ€»ç»“æ‰€æœ‰åˆ†æç»“æœï¼Œå¹¶æä¾›å…·ä½“çš„å»ºè®®å’Œæ”¹è¿›æ–¹æ¡ˆã€‚")
    ])
    
    round_prompts = [round1_prompt, round2_prompt, round3_prompt]
    
    print("=== ä½¿ç”¨å®Œæ•´å¯¹è¯è®°å¿† ===")
    results_buffer = run_multi_round_analysis_with_memory(
        docs, round_prompts, llm, memory_type="buffer"
    )
    
    print("\n=== ä½¿ç”¨çª—å£å¯¹è¯è®°å¿†ï¼ˆä¿ç•™æœ€è¿‘5è½®ï¼‰ ===")
    results_window = run_multi_round_analysis_with_memory(
        docs, round_prompts, llm, memory_type="window"
    )
    
    print("\n=== ä½¿ç”¨æ‘˜è¦å¯¹è¯è®°å¿† ===")
    results_summary = run_multi_round_analysis_with_memory(
        docs, round_prompts, llm, memory_type="summary"
    )
    
    return {
        "buffer_memory": results_buffer,
        "window_memory": results_window,
        "summary_memory": results_summary
    }